# Oracle Trinity Coordination for EU Funding

**Document ID:** EUFM-ORACLE-V1.0  
**Purpose:** How to leverage UBOS Trinity for maximum funding acquisition efficiency

---

## Overview

The UBOS Trinity (Claude + Codex + Gemini) operating through the Pneumatic Tube Network provides unprecedented capabilities for grant hunting. This document explains HOW to coordinate the three oracles for parallel, efficient funding acquisition.

**Key Insight:** Don't work sequentially. Work in PARALLEL. All three oracles operating simultaneously can complete in hours what would take weeks alone.

---

## The Oracle Trinity

### Claude: The Strategic Mind
**Vessel:** Anthropic Claude (Sonnet 4.5)  
**Location:** Primary strategic coordination  
**Strengths:**
- Long-horizon planning (30+ hour attention)
- Strategic synthesis across domains
- Blueprint thinking (the "why" behind actions)
- Narrative crafting and persuasion
- Constitutional alignment oversight

**Role in Funding:**
- Designs overall funding strategy
- Evaluates opportunities using scoring framework
- Synthesizes research into compelling narratives
- Crafts executive summaries and strategic sections
- Reviews for constitutional compliance

---

### Codex: The Code Forger
**Vessel:** OpenAI Codex / GPT-4  
**Location:** Code generation and technical documentation  
**Strengths:**
- Rapid code generation
- Technical documentation
- Structured data (tables, budgets, timelines)
- Template instantiation
- Automated formatting

**Role in Funding:**
- Generates budget tables and breakdowns
- Creates Gantt charts and timelines
- Produces work package structures
- Writes technical specifications
- Formats documents to portal requirements

---

### Gemini: The Systems Engineer
**Vessel:** Google Gemini  
**Location:** Infrastructure and technical feasibility  
**Strengths:**
- Systems architecture
- Technical feasibility analysis
- Infrastructure planning
- Consortium coordination
- Real-world deployment

**Role in Funding:**
- Validates technical feasibility
- Designs system architectures
- Plans infrastructure requirements
- Manages consortium building
- Ensures scalability

---

## External Oracle Network

Beyond the Trinity, leverage specialized oracles for funding intelligence:

### Perplexity: The Deep Researcher
**Access:** Via Perplexity API or web interface  
**Strengths:**
- Real-time web search and synthesis
- Recent program updates and deadlines
- Similar project discovery
- Success pattern extraction
- Contact point identification

**Use Cases:**
- Program-specific deep dives
- Winning project research
- Deadline verification
- Success rate research
- Evaluator profile research

---

### Groq: The Speed Scout
**Access:** Via Groq API (fast inference)  
**Strengths:**
- Ultra-fast processing
- Rapid keyword scanning
- Quick eligibility checks
- Bulk opportunity scanning

**Use Cases:**
- Initial opportunity scanning
- Deadline extraction across portals
- Quick keyword matching
- Rapid eligibility pre-screening

---

### Wolfram Alpha: The Quantifier
**Access:** Via Wolfram API  
**Strengths:**
- Precise calculations
- Scientific computations
- Unit conversions
- Statistical analysis
- Formula validation

**Use Cases:**
- ROI calculations
- Energy efficiency metrics
- CO2 reduction quantification
- Budget optimization
- Impact metric validation

---

### Data Commons: The Statistician
**Access:** Via Data Commons API (Google)  
**Strengths:**
- Official EU statistics
- Economic data
- Demographic information
- Environmental metrics
- Regional development data

**Use Cases:**
- Backing claims with EU official data
- Establishing baselines
- Demonstrating need quantitatively
- Comparing to EU benchmarks
- Regional analysis

---

## Coordination Patterns

### Pattern 1: Discovery Phase (Parallel Scan)

**Goal:** Find all relevant opportunities in 24-48 hours

```
YOU (Human) → Defines need clearly
              ↓
┌─────────────┴──────────────┐
│                             │
GROQ → Rapid portal scan    PERPLEXITY → Deep program research
  ↓                             ↓
  • All programs mentioning    • Detailed eligibility
    keywords                    • Success patterns
  • Deadlines extracted         • Contact points
  • Quick fit check             • Similar winners
              ↓
        CLAUDE → Synthesis & Scoring
              ↓
    Prioritized opportunity list
```

**Timeline:** 1-2 days  
**Output:** Scored and ranked opportunities

**Communication Method:**
```python
# Via Pneumatic Tube Network
puck = {
    "type": "research_request",
    "target_oracle": "groq",
    "query": "scan all EU portals for AI infrastructure grants",
    "urgency": "high",
    "deadline": "2025-11-04"
}
pucklib.pack(puck, recipient="groq")
```

---

### Pattern 2: Eligibility Phase (Parallel Validation)

**Goal:** Confirm eligibility in 24 hours

```
CLAUDE → Extracts requirements from official docs
    ↓
┌───┴───┐
│       │
GEMINI  CODEX → Validate technical/legal/financial criteria
│       │
└───┬───┘
    ↓
GAP ANALYSIS → Go/No-Go decision
```

**Timeline:** 1 day  
**Output:** Eligibility matrix with go/no-go recommendation

---

### Pattern 3: Documentation Phase (Parallel Generation)

**Goal:** Complete application in 3-7 days

```
YOU → Provides raw materials (financials, CVs, etc.)
    ↓
┌───┴────────────────┬─────────────────────┬───────────────┐
│                    │                     │               │
CLAUDE            CODEX               GEMINI         WOLFRAM + DATA COMMONS
↓                    ↓                     ↓               ↓
• Executive          • Budget tables       • Technical     • Statistical backing
  summary            • Timelines           specs           • ROI calculations
• Strategy           • Work packages       • Architecture  • Impact metrics
• Impact             • Deliverables        • Consortium    • Baseline data
• Alignment          • Risk matrix         management      • Projections
              ↓
        CLAUDE → Final assembly & review
              ↓
    Complete application package
```

**Timeline:** 3-7 days (first time), 1-2 days (subsequent)  
**Output:** Submission-ready application

---

### Pattern 4: Review Phase (Cross-Validation)

**Goal:** Catch errors before submission

```
Complete Draft
    ↓
┌───┴────┬────┬────┐
│        │    │    │
CLAUDE  CODEX GEMINI (each reviews from different angle)
↓        ↓    ↓    
Strategy Technical Feasibility
Check    Check   Check
│        │    │    
└───┬────┴────┴───┘
    ↓
Consolidated feedback → Revisions → Final version
```

**Timeline:** 1 day  
**Output:** Quality-assured application

---

## Specific Coordination Workflows

### Workflow A: Emergency Application (14-day deadline)

**Day 1-2:** Discovery (Groq + Perplexity parallel scan)  
**Day 3:** Eligibility (Gemini + Codex validation)  
**Day 4-10:** Documentation (full Trinity parallel generation)  
**Day 11-12:** Review and revision (cross-oracle validation)  
**Day 13:** Submission (Codex handles formatting)  
**Day 14:** Buffer for issues

**Oracle Utilization:** 90% parallel, 10% sequential

---

### Workflow B: Standard Application (4-week timeline)

**Week 1:**
- Days 1-2: Discovery (Perplexity deep research)
- Days 3-5: Eligibility and gap analysis (Gemini + Codex)
- Days 6-7: Material gathering (human + Codex organization)

**Week 2:**
- Days 8-10: Draft generation (full Trinity parallel)
- Days 11-14: First review and revision

**Week 3:**
- Days 15-17: Partner coordination (Gemini leads)
- Days 18-19: Quantitative validation (Wolfram + Data Commons)
- Days 20-21: Final assembly (Claude synthesis)

**Week 4:**
- Days 22-24: Cross-validation (all oracles review)
- Days 25-26: Final revisions
- Day 27: Submission
- Days 28-30: Buffer

**Oracle Utilization:** 70% parallel, 30% sequential

---

### Workflow C: Consortium Application (3-month timeline)

**Month 1: Partner Building**
- Weeks 1-2: Partner identification (Perplexity research + Gemini outreach)
- Weeks 3-4: Commitment securing (Claude narrative + human relationship)

**Month 2: Collaborative Documentation**
- Week 5: Kick-off and role assignment (Gemini coordinates)
- Weeks 6-7: Parallel section drafting (Trinity distributes work)
- Week 8: First integration (Claude synthesizes)

**Month 3: Review and Submission**
- Week 9: Partner review cycle (Gemini manages feedback)
- Week 10: Quantitative validation (Wolfram + Data Commons)
- Week 11: Final polish (Claude narrative refinement)
- Week 12: Submission and buffer

**Oracle Utilization:** 60% parallel, 40% coordination

---

## Research Request Templates

### For Perplexity (Deep Dive)

```markdown
## Research Request: [PROGRAM NAME]

**Objective:** Gather complete intelligence on [specific program]

**Required Information:**
1. Official program documentation URLs
2. Eligibility criteria (exhaustive list)
3. Typical success rate and funding amounts
4. Recent winning projects (3-5 examples with details)
5. Evaluation criteria and weights
6. Contact points and helpdesks
7. Application portal and registration requirements
8. Deadline and timeline information

**Output Format:** Structured markdown with sections for each item above

**Urgency:** [date needed by]

**Follow-up:** Flag any unclear areas requiring deeper research
```

Save this as: `00_SYSTEM/RESEARCH_REQUEST_TEMPLATE.md`

---

### For Groq (Fast Scan)

```markdown
## Rapid Scan Request: [TOPIC]

**Keywords:** [list of keywords]

**Portals to Scan:**
- Funding & Tenders Portal (EU)
- Horizon Europe Work Programme
- Digital Europe Programme
- Innovation Fund
- [Country-specific portals]

**Extract:**
- Program names
- Budget amounts
- Deadlines
- Quick eligibility match (yes/no/maybe)

**Output:** JSON array of opportunities with above fields

**Time Limit:** 2 hours maximum
```

---

### For Wolfram (Quantification)

```markdown
## Calculation Request: [METRIC NAME]

**Formula:** [if known, provide formula]

**Input Data:**
- [list all input parameters]

**Required Precision:** [decimal places]

**Validation:** Cross-check against [benchmark/standard]

**Output:** Calculated value + methodology explanation + confidence level
```

---

### For Data Commons (Statistical Backing)

```markdown
## Data Request: [METRIC/CLAIM]

**Claim to Back:** [specific claim from proposal]

**Required Statistics:**
- Geographic scope: [EU-wide, country, region]
- Time period: [years]
- Data points: [specific metrics needed]

**Official Source Preference:** Eurostat > National Stats > Other

**Output:** Data values + source URLs + download date + provenance chain
```

---

## Communication Protocols

### Via Pneumatic Tube Network

**Structure:** All inter-oracle messages use the `pucklib` format

```python
mission_brief = {
    "mission_id": generate_id(),
    "from_vessel": "claude",
    "to_vessel": "codex",
    "mission_type": "document_generation",
    "priority": 2,  # 1=urgent, 2=normal, 3=low
    "payload": {
        "task": "Generate budget table for PNRR application",
        "inputs": {
            "total_budget": 20000,
            "eu_funding": 18000,
            "co_financing": 2000,
            "categories": ["hardware", "training", "consulting"]
        },
        "output_format": "markdown_table",
        "deadline": "2025-11-05T18:00:00Z"
    }
}

# Send via pneumatic tube
pucklib.pack(mission_brief, recipient="codex")

# Codex receives, processes, sends response
response = pucklib.unpack(inbox="claude")
```

**Location:** `03_OPERATIONS/COMMS_HUB/`

---

## Best Practices

### DO:
✅ Start all phases with parallel oracle deployment  
✅ Provide clear, specific requests to each oracle  
✅ Use structured formats (JSON, markdown tables)  
✅ Set realistic deadlines for each oracle task  
✅ Cross-validate critical information across oracles  
✅ Document oracle coordination in project folder  

### DON'T:
❌ Work sequentially when parallel possible  
❌ Give vague or open-ended requests  
❌ Skip cross-validation of quantitative claims  
❌ Overload any single oracle  
❌ Forget to track who generated what (provenance!)  
❌ Use oracles for tasks humans do better (relationship building)  

---

## Measuring Oracle Efficiency

**Track these metrics:**

**Speed:** Time from request to usable output  
**Quality:** How often oracle outputs require major revision  
**Accuracy:** Oracle predictions vs. actual outcomes  
**Cost:** API costs per funding application  
**ROI:** Funding secured vs. oracle costs + human time  

**Goal:** Each application should refine oracle coordination, making next application faster and cheaper.

---

## Integration with UBOS Mission System

Oracle coordination for funding pursuits integrates with the broader UBOS mission orchestration:

**Mission Creation:** Define funding pursuit as mission in `03_OPERATIONS/COMMS_HUB/missions/`

**Task Assignment:** Break mission into oracle-specific tasks

**Progress Tracking:** Monitor via mission status updates

**Deliverable Assembly:** Combine oracle outputs into final submission

**Details:** See main UBOS operational documentation

---

## Version History

**v1.0 (November 2025):** Initial oracle coordination framework for EU funding applications.

---

**Remember:** The oracles are not replacements for human judgment and relationships. They're force multipliers. Use them for research, drafting, calculation, and validation - but YOU provide strategy, relationships, and final decisions.

**Coordinate wisely. Build systematically. Compound intelligence.** ⚙️

